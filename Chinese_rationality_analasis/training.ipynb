{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial for Chinese Sentiment analysis with hotel review data\n",
    "## Dependencies\n",
    "\n",
    "Python 3.5, numpy, pickle, keras, tensorflow, [jieba](https://github.com/fxsjy/jieba)\n",
    "\n",
    "## Optional for plotting\n",
    "\n",
    "pylab, scipy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yingshaoxo/.local/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import jieba\n",
    "import codecs\n",
    "from langconv import * # convert Traditional Chinese characters to Simplified Chinese characters\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import GRU\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers.core import Dense\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper function to pickle and load stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def __pickleStuff(filename, stuff):\n",
    "    save_stuff = open(filename, \"wb\")\n",
    "    pickle.dump(stuff, save_stuff)\n",
    "    save_stuff.close()\n",
    "def __loadStuff(filename):\n",
    "    saved_stuff = open(filename,\"rb\")\n",
    "    stuff = pickle.load(saved_stuff)\n",
    "    saved_stuff.close()\n",
    "    return stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get lists of files, positive and negative files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataBaseDirPos = \"./Data/positive/\"\n",
    "dataBaseDirNeg = \"./Data/negative/\"\n",
    "positiveFiles = [dataBaseDirPos + f for f in listdir(dataBaseDirPos) if isfile(join(dataBaseDirPos, f)) and '.txt' in f]\n",
    "negativeFiles = [dataBaseDirNeg + f for f in listdir(dataBaseDirNeg) if isfile(join(dataBaseDirNeg, f)) and '.txt' in f]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show length of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "4\n",
      "\n",
      "['./Data/positive/diary.txt', './Data/positive/msgs.txt', './Data/positive/theory.txt', './Data/positive/mind.txt', './Data/positive/drafts.txt', './Data/positive/saying.txt']\n",
      "['./Data/negative/QQZoneComments.txt', './Data/negative/DuanZi.txt', './Data/negative/SiBuDeJieDianzi.txt', './Data/negative/BilibiliComments.txt']\n"
     ]
    }
   ],
   "source": [
    "print(len(positiveFiles))\n",
    "print(len(negativeFiles))\n",
    "\n",
    "print()\n",
    "print(positiveFiles)\n",
    "print(negativeFiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Have a look at what's in a file(one hotel review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "在这个世界上我能活多久？是空留无一物还是另类？我不知道，也不会去想。\n",
      "\n",
      "世界总是要我们给予什么，但残酷的命运无情的夺走我们的一切。\n",
      "\n",
      "时间在这时已停止，只留下一串串时间的印记串联起的文字。\n",
      "\n",
      "因此才有了这本日记，他是属于自己的，没人偷看。\n",
      "\n",
      "这是一片自由的天空，任自己遨游，飞跃时间的限制，让我们能在年老的时候说：瞧！这就是青春，我的宝贵时间就是那样过的！\n",
      "\n",
      "——————————————\n",
      "\n",
      "天空一如\n"
     ]
    }
   ],
   "source": [
    "filename = positiveFiles[0]\n",
    "with codecs.open(filename, \"r\", encoding=\"utf-8\", errors=\"ignore\") as doc_file:\n",
    "    text=doc_file.read()\n",
    "    print(text[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test removing stop words\n",
    "Demo what it looks like to tokenize the sentence and remove stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==Orginal==:\n",
      "\r",
      "我对垃圾的断绝能力一直很低导致我在现实中经常很不爽要是拒绝可以更坚决一点，就没那么多伤害了——————————————喜剧之王 一点都不好看——————————————构建一套系统真的没那么容易比如 找工作APP如何构建一个诚信机制，既能让没有任何认证的人找到工作，又不让企业吃亏(淘宝是怎么做的？让人数少的想赚钱的商家交保证金，人数多的消费者不\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.654 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==Tokenized==\tToken count:119\n",
      "\r",
      "我 对 垃圾 的 断绝 能力 一直 很 低 导致 我 在 现实 中 经常 很 不爽 要是 拒绝 可以 更 坚决 一点 ， 就 没 那么 多 伤害 了 — — — — — — — — — — — — — — 喜剧之王   一点 都 不 好看 — — — — — — — — — — — — — — 构建 一套 系统 真的 没 那么 容易 比如   找 工作 APP 如何 构建 一个 诚信 机制 ， 既能 让 没有 任何 认证 的 人 找到 工作 ， 又 不让 企业 吃亏 ( 淘宝 是 怎么 做 的 ？ 让 人数 少 的 想 赚钱 的 商家 交 保证金 ， 人数 多 的 消费者 不\n",
      "==Stop Words Removed==\tToken count:44\n",
      "\r",
      "垃圾 断绝 能力 低 导致 现实 中 不爽 拒绝 一点 伤害 喜剧之王   一点 好看 构建 一套 系统 真的   找 工作 APP 构建 诚信 机制 既能 认证 找到 工作 不让 企业 吃亏 淘宝 做 人数 少 想 赚钱 商家 交 保证金 人数 消费者\n"
     ]
    }
   ],
   "source": [
    "filename = positiveFiles[1]\n",
    "with codecs.open(filename, \"r\", encoding=\"utf-8\", errors=\"ignore\") as doc_file:\n",
    "    text=doc_file.read()[:200]\n",
    "    text = text.replace(\"\\n\", \"\")\n",
    "    text = text.replace(\"\\r\", \"\")\n",
    "print(\"==Orginal==:\\n\\r{}\".format(text))\n",
    "    \n",
    "stopwords = [ line.rstrip() for line in codecs.open('./Data/chinese_stop_words.txt',\"r\", encoding=\"utf-8\") ]\n",
    "seg_list = jieba.cut(text, cut_all=False)\n",
    "final =[]\n",
    "seg_list = list(seg_list)\n",
    "for seg in seg_list:\n",
    "    if seg not in stopwords:\n",
    "        final.append(seg)\n",
    "print(\"==Tokenized==\\tToken count:{}\\n\\r{}\".format(len(seg_list),\" \".join(seg_list)))\n",
    "print(\"==Stop Words Removed==\\tToken count:{}\\n\\r{}\".format(len(final),\" \".join(final)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare \"doucments\", a list of tuples\n",
    "Some files contain abnormal encoding characters which encoding GB2312 will complain about. Solution: read as bytes then decode as GB2312 line by line, skip lines with abnormal encodings. We also convert any traditional Chinese characters to simplified Chinese characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive_nums: 8739\n",
      "negative_nums: 13422\n"
     ]
    }
   ],
   "source": [
    "documents = []\n",
    "positive_nums = 0\n",
    "negative_nums = 0\n",
    "\n",
    "for filename in positiveFiles:\n",
    "    with open(filename, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        text = f.read()\n",
    "    all_text = Converter('zh-hans').convert(text)# Convert from traditional to simplified Chinese\n",
    "    text_list = all_text.split(\"\\n\\n——————————————\\n\\n\")\n",
    "    for text in text_list:\n",
    "        #text = text.replace(\"\\n\", \"\")\n",
    "        #text = text.replace(\"\\r\", \"\")\n",
    "        documents.append((text, \"pos\"))\n",
    "        positive_nums += 1\n",
    "\n",
    "for filename in negativeFiles:\n",
    "    with open(filename, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        text = f.read()\n",
    "    all_text = Converter('zh-hans').convert(text)# Convert from traditional to simplified Chinese\n",
    "    text_list = all_text.split(\"\\n\\n——————————————\\n\\n\")\n",
    "    for text in text_list:\n",
    "        #text = text.replace(\"\\n\", \"\")\n",
    "        #text = text.replace(\"\\r\", \"\")\n",
    "        documents.append((text, \"neg\"))\n",
    "        negative_nums += 1\n",
    "\n",
    "print('positive_nums:', positive_nums)\n",
    "print('negative_nums:', negative_nums)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional step to save/load the documents as pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22161\n",
      "[('每天都做，但还没研究过，现在好了哈哈', 'neg'), ('极限6分钟，四分钟开始全身抖动', 'neg'), ('(=・ω・=)', 'neg')]\n"
     ]
    }
   ],
   "source": [
    "# Uncomment those two lines to save/load the documents for later use since the step above takes a while\n",
    "# __pickleStuff(\"./Data/chinese_sentiment_corpus.p\", documents)\n",
    "# documents = __loadStuff(\"./Data/chinese_sentiment_corpus.p\")\n",
    "print(len(documents))\n",
    "print(documents[-4:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## shuffle the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the input and output for the model\n",
    "Each input (hotel review) will be a list of tokens, output will be one token(\"pos\" or \"neg\"). The stopwords are not removed here since the dataset is relative small and removing the stop words are not saving much traing time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize only\n",
    "totalX = []\n",
    "totalY = [str(doc[1]) for doc in documents]\n",
    "for doc in documents:\n",
    "    seg_list = jieba.cut(doc[0], cut_all=False)\n",
    "    seg_list = list(seg_list)\n",
    "    totalX.append(seg_list)\n",
    "\n",
    "\n",
    "#Switch to below code to experiment with removing stop words\n",
    "# Tokenize and remove stop words\n",
    "# totalX = []\n",
    "# totalY = [str(doc[1]) for doc in documents]\n",
    "# stopwords = [ line.rstrip() for line in codecs.open('./Data/chinese_stop_words.txt',\"r\", encoding=\"utf-8\") ]\n",
    "# for doc in documents:\n",
    "#     seg_list = jieba.cut(doc[0], cut_all=False)\n",
    "#     seg_list = list(seg_list)\n",
    "#     Uncomment below code to experiment with removing stop words\n",
    "#     final =[]\n",
    "#     for seg in seg_list:\n",
    "#         if seg not in stopwords:\n",
    "#             final.append(seg)\n",
    "#     totalX.append(final)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize distribution of sentence length\n",
    "Decide the max input sequence, here we cover up to 60% sentences. The longer input sequence, the more training time will take, but could improve  prediction accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length is:  2677\n",
      "60% cover length up to:  16\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f95b53d88d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import scipy.stats as stats\n",
    "import pylab as pl\n",
    "h = sorted([len(sentence) for sentence in totalX])\n",
    "maxLength = h[int(len(h) * 0.60)]\n",
    "print(\"Max length is: \",h[len(h)-1])\n",
    "print(\"60% cover length up to: \",maxLength)\n",
    "h = h[:5000]\n",
    "fit = stats.norm.pdf(h, np.mean(h), np.std(h))  #this is a fitting indeed\n",
    "\n",
    "pl.plot(h,fit,'-o')\n",
    "pl.hist(h,normed=True)      #use this to draw histogram of your data\n",
    "pl.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Words to number tokens, padding\n",
    "Pad input sequence to max input length if it is shorter\n",
    "\n",
    "\n",
    "Save the input tokenizer, since we need to use the same tokenizer for our new predition data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input vocab_size: 44932\n"
     ]
    }
   ],
   "source": [
    "totalX = [\" \".join(wordslist) for wordslist in totalX]  # Keras Tokenizer expect the words tokens to be seperated by space \n",
    "input_tokenizer = Tokenizer(30000) # Initial vocab size\n",
    "input_tokenizer.fit_on_texts(totalX)\n",
    "vocab_size = len(input_tokenizer.word_index) + 1\n",
    "print(\"input vocab_size:\",vocab_size)\n",
    "totalX = np.array(pad_sequences(input_tokenizer.texts_to_sequences(totalX), maxlen=maxLength))\n",
    "__pickleStuff(\"./Data/input_tokenizer_chinese.p\", input_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output, array of 0s and 1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output vocab_size: 3\n"
     ]
    }
   ],
   "source": [
    "target_tokenizer = Tokenizer(3)\n",
    "target_tokenizer.fit_on_texts(totalY)\n",
    "print(\"output vocab_size:\",len(target_tokenizer.word_index) + 1)\n",
    "totalY = np.array(target_tokenizer.texts_to_sequences(totalY)) -1\n",
    "totalY = totalY.reshape(totalY.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 0, 0, 1, 0, 0, 1, 0])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totalY[40:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turn output 0s and 1s to categories(one-hot vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalY = to_categorical(totalY, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totalY[40:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dimen = totalY.shape[1] # which is 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save meta data for later predition\n",
    "maxLength: the input sequence length\n",
    "\n",
    "vocab_size: Input vocab size\n",
    "\n",
    "output_dimen: which is 2 in this example (pos or neg)\n",
    "\n",
    "sentiment_tag: either [\"neg\",\"pos\"] or [\"pos\",\"neg\"] matching the target tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_reverse_word_index = {v: k for k, v in list(target_tokenizer.word_index.items())}\n",
    "sentiment_tag = [target_reverse_word_index[1],target_reverse_word_index[2]] \n",
    "metaData = {\"maxLength\":maxLength,\"vocab_size\":vocab_size,\"output_dimen\":output_dimen,\"sentiment_tag\":sentiment_tag}\n",
    "__pickleStuff(\"./Data/meta_sentiment_chinese.p\", metaData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Model, train and save it\n",
    "The training data is logged to Tensorboard, we can look at it by cd into directory \n",
    "\n",
    "\"./Graph/sentiment_chinese\" and run\n",
    "\n",
    "\n",
    "\"python -m tensorflow.tensorboard --logdir=.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 16, 256)           11502592  \n",
      "_________________________________________________________________\n",
      "gru_5 (GRU)                  (None, 16, 256)           393984    \n",
      "_________________________________________________________________\n",
      "gru_6 (GRU)                  (None, 256)               393984    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 12,291,074\n",
      "Trainable params: 12,291,074\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 19944 samples, validate on 2217 samples\n",
      "Epoch 1/20\n",
      "19944/19944 [==============================] - 136s 7ms/step - loss: 0.4405 - acc: 0.7962 - val_loss: 0.2880 - val_acc: 0.8787\n",
      "Epoch 2/20\n",
      "19944/19944 [==============================] - 132s 7ms/step - loss: 0.2691 - acc: 0.8924 - val_loss: 0.2263 - val_acc: 0.9143\n",
      "Epoch 3/20\n",
      "19944/19944 [==============================] - 132s 7ms/step - loss: 0.2026 - acc: 0.9237 - val_loss: 0.2075 - val_acc: 0.9193\n",
      "Epoch 4/20\n",
      "19944/19944 [==============================] - 133s 7ms/step - loss: 0.1709 - acc: 0.9371 - val_loss: 0.2184 - val_acc: 0.9224\n",
      "Epoch 5/20\n",
      "19944/19944 [==============================] - 134s 7ms/step - loss: 0.1408 - acc: 0.9504 - val_loss: 0.2202 - val_acc: 0.9260\n",
      "Epoch 6/20\n",
      "19944/19944 [==============================] - 134s 7ms/step - loss: 0.1258 - acc: 0.9559 - val_loss: 0.2233 - val_acc: 0.9197\n",
      "Epoch 7/20\n",
      "19944/19944 [==============================] - 134s 7ms/step - loss: 0.1066 - acc: 0.9622 - val_loss: 0.2235 - val_acc: 0.9157\n",
      "Epoch 8/20\n",
      "19944/19944 [==============================] - 134s 7ms/step - loss: 0.0943 - acc: 0.9662 - val_loss: 0.2344 - val_acc: 0.9188\n",
      "Epoch 9/20\n",
      "19944/19944 [==============================] - 134s 7ms/step - loss: 0.0839 - acc: 0.9713 - val_loss: 0.2617 - val_acc: 0.9188\n",
      "Epoch 10/20\n",
      "19944/19944 [==============================] - 135s 7ms/step - loss: 0.0787 - acc: 0.9717 - val_loss: 0.2537 - val_acc: 0.9179\n",
      "Epoch 11/20\n",
      "19944/19944 [==============================] - 135s 7ms/step - loss: 0.0692 - acc: 0.9756 - val_loss: 0.2945 - val_acc: 0.9179\n",
      "Epoch 12/20\n",
      "19944/19944 [==============================] - 135s 7ms/step - loss: 0.0604 - acc: 0.9788 - val_loss: 0.2801 - val_acc: 0.9197\n",
      "Epoch 13/20\n",
      "19944/19944 [==============================] - 135s 7ms/step - loss: 0.0610 - acc: 0.9776 - val_loss: 0.3039 - val_acc: 0.9175\n",
      "Epoch 14/20\n",
      "19944/19944 [==============================] - 135s 7ms/step - loss: 0.0571 - acc: 0.9785 - val_loss: 0.2796 - val_acc: 0.9202\n",
      "Epoch 15/20\n",
      "19944/19944 [==============================] - 134s 7ms/step - loss: 0.0524 - acc: 0.9816 - val_loss: 0.3366 - val_acc: 0.9152\n",
      "Epoch 16/20\n",
      "19944/19944 [==============================] - 135s 7ms/step - loss: 0.0490 - acc: 0.9811 - val_loss: 0.3833 - val_acc: 0.9161\n",
      "Epoch 17/20\n",
      "19944/19944 [==============================] - 135s 7ms/step - loss: 0.0470 - acc: 0.9832 - val_loss: 0.4218 - val_acc: 0.9102\n",
      "Epoch 18/20\n",
      "19944/19944 [==============================] - 135s 7ms/step - loss: 0.0437 - acc: 0.9838 - val_loss: 0.3720 - val_acc: 0.9080\n",
      "Epoch 19/20\n",
      "19944/19944 [==============================] - 135s 7ms/step - loss: 0.0430 - acc: 0.9843 - val_loss: 0.4309 - val_acc: 0.9044\n",
      "Epoch 20/20\n",
      "19944/19944 [==============================] - 135s 7ms/step - loss: 0.0426 - acc: 0.9850 - val_loss: 0.3375 - val_acc: 0.9066\n",
      "Saved model!\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 256\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim,input_length = maxLength))\n",
    "# Each input would have a size of (maxLength x 256) and each of these 256 sized vectors are fed into the GRU layer one at a time.\n",
    "# All the intermediate outputs are collected and then passed on to the second GRU layer.\n",
    "model.add(GRU(256, dropout=0.9, return_sequences=True))\n",
    "# Using the intermediate outputs, we pass them to another GRU layer and collect the final output only this time\n",
    "model.add(GRU(256, dropout=0.9))\n",
    "# The output is then sent to a fully connected layer that would give us our final output_dim classes\n",
    "model.add(Dense(output_dimen, activation='softmax'))\n",
    "# We use the adam optimizer instead of standard SGD since it converges much faster\n",
    "tbCallBack = TensorBoard(log_dir='./Graph/sentiment_chinese', histogram_freq=0,\n",
    "                            write_graph=True, write_images=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "model.fit(totalX, totalY, validation_split=0.1, batch_size=32, epochs=20, verbose=1, callbacks=[tbCallBack])\n",
    "model.save('./Data/sentiment_chinese_model.HDF5')\n",
    "\n",
    "print(\"Saved model!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below are prediction code\n",
    "Function to load the meta data and the model we just trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None\n",
    "sentiment_tag = None\n",
    "maxLength = None\n",
    "def loadModel():\n",
    "    global model, sentiment_tag, maxLength\n",
    "    metaData = __loadStuff(\"./Data/meta_sentiment_chinese.p\")\n",
    "    maxLength = metaData.get(\"maxLength\")\n",
    "    vocab_size = metaData.get(\"vocab_size\")\n",
    "    output_dimen = metaData.get(\"output_dimen\")\n",
    "    sentiment_tag = metaData.get(\"sentiment_tag\")\n",
    "    embedding_dim = 256\n",
    "    if model is None:\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(vocab_size, embedding_dim, input_length=maxLength))\n",
    "        # Each input would have a size of (maxLength x 256) and each of these 256 sized vectors are fed into the GRU layer one at a time.\n",
    "        # All the intermediate outputs are collected and then passed on to the second GRU layer.\n",
    "        model.add(GRU(256, dropout=0.9, return_sequences=True))\n",
    "        # Using the intermediate outputs, we pass them to another GRU layer and collect the final output only this time\n",
    "        model.add(GRU(256, dropout=0.9))\n",
    "        # The output is then sent to a fully connected layer that would give us our final output_dim classes\n",
    "        model.add(Dense(output_dimen, activation='softmax'))\n",
    "        # We use the adam optimizer instead of standard SGD since it converges much faster\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        model.load_weights('./Data/sentiment_chinese_model.HDF5')\n",
    "        model.summary()\n",
    "    print(\"Model weights loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to convert sentence to model input, and predict result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findFeatures(text):\n",
    "    text=Converter('zh-hans').convert(text)\n",
    "    text = text.replace(\"\\n\", \"\")\n",
    "    text = text.replace(\"\\r\", \"\") \n",
    "    seg_list = jieba.cut(text, cut_all=False)\n",
    "    seg_list = list(seg_list)\n",
    "    text = \" \".join(seg_list)\n",
    "    textArray = [text]\n",
    "    input_tokenizer_load = __loadStuff(\"./Data/input_tokenizer_chinese.p\")\n",
    "    textArray = np.array(pad_sequences(input_tokenizer_load.texts_to_sequences(textArray), maxlen=maxLength))\n",
    "    return textArray\n",
    "def predictResult(text):\n",
    "    if model is None:\n",
    "        print(\"Please run \\\"loadModel\\\" first.\")\n",
    "        return None\n",
    "    features = findFeatures(text)\n",
    "    predicted = model.predict(features)[0] # we have only one sentence to predict, so take index 0\n",
    "    predicted = np.array(predicted)\n",
    "    probab = predicted.max()\n",
    "    predition = sentiment_tag[predicted.argmax()]\n",
    "    return predition, probab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calling the load model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 16, 256)           11502592  \n",
      "_________________________________________________________________\n",
      "gru_7 (GRU)                  (None, 16, 256)           393984    \n",
      "_________________________________________________________________\n",
      "gru_8 (GRU)                  (None, 256)               393984    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 12,291,074\n",
      "Trainable params: 12,291,074\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model weights loaded!\n"
     ]
    }
   ],
   "source": [
    "loadModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try some new comments, feel free to try your own\n",
    "The result tuple consists the predicted result and likehood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('neg', 0.99519217)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictResult(\"还好，床很大而且很干净，前台很友好，很满意，下次还来。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('neg', 0.9960509)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictResult(\"房间有点小但是设备还齐全，没有异味。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('neg', 0.99629444)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictResult(\"房间还算干净，一般般吧，短住还凑合。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('neg', 0.999734)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictResult(\"开始不太满意，前台好说话换了一间，房间很干净没有异味。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('pos', 0.7794182)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictResult(\"以前从没有出现过这种情况，这一定有问题\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('pos', 0.99968886)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictResult(\"需求决定人的行为\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('pos', 0.96061784)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictResult(\"我不同意你所说的每一个字，但我誓死捍卫你说话的权力\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('pos', 0.99474674)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictResult(\"凡夫俗子只关心如何去打发时间，而略具才华的人却考虑如何应用时间\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('neg', 0.7898073)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictResult(\"清华大学的傻逼们，请出来说句话\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('neg', 0.99981683)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictResult(\"我好可怜奥\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('neg', 0.9994141)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictResult(\"好久都没有听到一首这样有韵味的歌了！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('pos', 0.7159147)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictResult(\"在一个傍晚的偏远小镇上，街道上寒冷凄清，几乎看不到路人，只有几盏闪烁的霓虹灯，渲染着寂寥的风景。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('neg', 0.99978024)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictResult(\"走开，女大十八变不知道啊\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('neg', 0.9944026)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictResult(\"踢个球右腿被干了，瓜皮瓜皮\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('neg', 0.92771965)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictResult(\"终于他梁的忙完这些稀里糊涂的东西了，爆炸\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('pos', 0.85333896)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictResult(\"大家都是平等的\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "never give up \n",
      " ('pos', 0.99935156) \n",
      "\n",
      "\n",
      "I'm born to do this \n",
      " ('pos', 0.99883515) \n",
      "\n",
      "\n",
      "有希望在的地方，痛苦也成欢乐。 \n",
      " ('pos', 0.8465268) \n",
      "\n",
      "\n",
      "信仰是人生杠杆的支撑点，具备这个支撑点，才可能成为一个强而有力的人；信仰是事业的大门，没有正确的信仰，注定做不出伟大的事业。 \n",
      " ('pos', 0.9992962) \n",
      "\n",
      "\n",
      "哲学是有严密逻辑系统的宇宙观，它研究宇宙的性质、宇宙内万事万物演化的总规律、人在宇宙中的位置等等一些很基本的问题 \n",
      " ('pos', 0.8127168) \n",
      "\n",
      "\n",
      "伟人与平凡人的差别在于，伟人的胸中并不是没有不自信的时候，只是他能够在不自信时调整自己，从而从不自信中走出来，以达到自信的旺盛的精神状态 \n",
      " ('pos', 0.9989254) \n",
      "\n",
      "\n",
      "别人是自己的镜子，自己应该在别人成功与失败的教训中避免不幸的重现。 \n",
      " ('pos', 0.9990814) \n",
      "\n",
      "\n",
      "劣书是损害我们精神思想的毒药。 \n",
      " ('pos', 0.997758) \n",
      "\n",
      "\n",
      "I love losing face \n",
      " ('pos', 0.99403834) \n",
      "\n",
      "\n",
      "陈述性的讲演不会被当成 negative \n",
      " ('pos', 0.7932613) \n",
      "\n",
      "\n",
      "偏激的、平庸的、不讲逻辑的才会 \n",
      " ('pos', 0.99915993) \n",
      "\n",
      "\n",
      "生死狙击是这两年兴起的一款页游 \n",
      " ('pos', 0.9741) \n",
      "\n",
      "\n",
      "A teacher from a community college addressed a sympathetic audience. \n",
      " ('neg', 0.9739342) \n",
      "\n",
      "\n",
      "你怕是个傻子 \n",
      " ('neg', 0.99875474) \n",
      "\n",
      "\n",
      "好耶好耶，妈妈有爸爸了 \n",
      " ('neg', 0.9998623) \n",
      "\n",
      "\n",
      "小学生们要喷就喷点有营养的好么 \n",
      " ('neg', 0.99983287) \n",
      "\n",
      "\n",
      "SB游戏 \n",
      " ('neg', 0.8586241) \n",
      "\n",
      "\n",
      "本人玩这个英雄联盟也有几千场了，打这么多场下来，不说100%的场次， 至少90%的场次是属于以下类型的。1,己方3路全爆或者敌方3路全爆2.赢是躺赢，输是凯瑞。3一方默契到爆每 \n",
      " ('neg', 0.8407083) \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "saying = \"\"\"\n",
    "never give up\n",
    "I'm born to do this\n",
    "有希望在的地方，痛苦也成欢乐。\n",
    "信仰是人生杠杆的支撑点，具备这个支撑点，才可能成为一个强而有力的人；信仰是事业的大门，没有正确的信仰，注定做不出伟大的事业。\n",
    "哲学是有严密逻辑系统的宇宙观，它研究宇宙的性质、宇宙内万事万物演化的总规律、人在宇宙中的位置等等一些很基本的问题\n",
    "伟人与平凡人的差别在于，伟人的胸中并不是没有不自信的时候，只是他能够在不自信时调整自己，从而从不自信中走出来，以达到自信的旺盛的精神状态\n",
    "别人是自己的镜子，自己应该在别人成功与失败的教训中避免不幸的重现。\n",
    "劣书是损害我们精神思想的毒药。\n",
    "I love losing face\n",
    "陈述性的讲演不会被当成 negative\n",
    "偏激的、平庸的、不讲逻辑的才会\n",
    "生死狙击是这两年兴起的一款页游\n",
    "\n",
    " \n",
    "A teacher from a community college addressed a sympathetic audience.\n",
    "你怕是个傻子\n",
    "好耶好耶，妈妈有爸爸了\n",
    "小学生们要喷就喷点有营养的好么\n",
    "SB游戏\n",
    "本人玩这个英雄联盟也有几千场了，打这么多场下来，不说100%的场次， 至少90%的场次是属于以下类型的。1,己方3路全爆或者敌方3路全爆2.赢是躺赢，输是凯瑞。3一方默契到爆每次抓人先人一步，或者无脑团，每次团得比对方快几秒。这个游戏秒人速度大家是有目共睹的，任何一个小小的失误都会导致被秒，团灭或者队友之间的胡喷，而且请记住，你是绝对无法彻底控制一场对战的随机性的。在这个战局优劣瞬息万变的游戏，5个随机的人打另外5个随机的人，又有各式各样的阵容克制，单个英雄之间的克制，还有暴击率。在这样一个随机性游戏里面，概率事件变得如此之多的游戏，很有可能这个游戏需要的运气量比你打牌或者赌钱的运气更多，前提是运气能量化的话。能决定你输或者赢得跟你技术关系真不大，不管你是翻盘局，少胜多，还是你凯瑞了，或者你带崩全局。都说明不了你，你队友或者你对手很垃圾或者很NB。综上经常开比赛，描述英雄联盟是一个多需要技术多注重竞技性的游戏，来洗脑这个只能玩路人局的你，舔着B脸说自己是竞技游戏的，真的是太垃圾了。\n",
    "\"\"\"\n",
    "text_list = [text for text in saying.split('\\n') if text.strip('\\n ') != '']\n",
    "for text in text_list:\n",
    "    print(text[:88], '\\n', predictResult(text), '\\n'*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
