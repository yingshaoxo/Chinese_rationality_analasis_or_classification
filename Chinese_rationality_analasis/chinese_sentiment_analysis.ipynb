{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial for Chinese Sentiment analysis with hotel review data\n",
    "## Dependencies\n",
    "\n",
    "Python 3.5, numpy, pickle, keras, tensorflow, [jieba](https://github.com/fxsjy/jieba)\n",
    "\n",
    "## Optional for plotting\n",
    "\n",
    "pylab, scipy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import jieba\n",
    "import codecs\n",
    "from langconv import * # convert Traditional Chinese characters to Simplified Chinese characters\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import GRU\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers.core import Dense\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper function to pickle and load stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def __pickleStuff(filename, stuff):\n",
    "    save_stuff = open(filename, \"wb\")\n",
    "    pickle.dump(stuff, save_stuff)\n",
    "    save_stuff.close()\n",
    "def __loadStuff(filename):\n",
    "    saved_stuff = open(filename,\"rb\")\n",
    "    stuff = pickle.load(saved_stuff)\n",
    "    saved_stuff.close()\n",
    "    return stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get lists of files, positive and negative files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataBaseDirPos = \"./data/ChnSentiCorp_htl_ba_6000/pos/\"\n",
    "dataBaseDirNeg = \"./data/ChnSentiCorp_htl_ba_6000/neg/\"\n",
    "positiveFiles = [dataBaseDirPos + f for f in listdir(dataBaseDirPos) if isfile(join(dataBaseDirPos, f))]\n",
    "negativeFiles = [dataBaseDirNeg + f for f in listdir(dataBaseDirNeg) if isfile(join(dataBaseDirNeg, f))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show length of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2916\n",
      "3000\n"
     ]
    }
   ],
   "source": [
    "print(len(positiveFiles))\n",
    "print(len(negativeFiles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Have a look at what's in a file(one hotel review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "如果是我一个人再到东莞出差的话，我还会住在东莞宾馆，但是如果我和我的老板一起来东莞的话，可能会住到索菲特去。\r",
      "\r\n",
      "我喜欢东莞宾馆的环境和它的几个餐厅，身处闹市但是却很安静，很舒服，宾馆的服务也很到位。\r",
      "\r\n",
      "对于普通的商务旅行者来说，东莞宾馆的标准间是最物超所值的。\r",
      "\r\n",
      "\r\n",
      "\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filename = positiveFiles[0]\n",
    "with codecs.open(filename, \"r\", encoding=\"gb2312\") as doc_file:\n",
    "    text=doc_file.read()\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test removing stop words\n",
    "Demo what it looks like to tokenize the sentence and remove stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==Orginal==:\n",
      "\r",
      "别墅型的酒店，非常特别,离海边很近.消费很平价\n",
      "==Tokenized==\tToken count:15\n",
      "\r",
      "别墅 型 的 酒店 ， 非常 特别 , 离 海边 很近 . 消费 很 平价\n",
      "==Stop Words Removed==\tToken count:8\n",
      "\r",
      "别墅 型 酒店 特别 海边 很近 消费 平价\n"
     ]
    }
   ],
   "source": [
    "filename = positiveFiles[110]\n",
    "with codecs.open(filename, \"r\", encoding=\"gb2312\") as doc_file:\n",
    "    text=doc_file.read()\n",
    "    text = text.replace(\"\\n\", \"\")\n",
    "    text = text.replace(\"\\r\", \"\")\n",
    "print(\"==Orginal==:\\n\\r{}\".format(text))\n",
    "    \n",
    "stopwords = [ line.rstrip() for line in codecs.open('./data/chinese_stop_words.txt',\"r\", encoding=\"utf-8\") ]\n",
    "seg_list = jieba.cut(text, cut_all=False)\n",
    "final =[]\n",
    "seg_list = list(seg_list)\n",
    "for seg in seg_list:\n",
    "    if seg not in stopwords:\n",
    "        final.append(seg)\n",
    "print(\"==Tokenized==\\tToken count:{}\\n\\r{}\".format(len(seg_list),\" \".join(seg_list)))\n",
    "print(\"==Stop Words Removed==\\tToken count:{}\\n\\r{}\".format(len(final),\" \".join(final)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare \"doucments\", a list of tuples\n",
    "Some files contain abnormal encoding characters which encoding GB2312 will complain about. Solution: read as bytes then decode as GB2312 line by line, skip lines with abnormal encodings. We also convert any traditional Chinese characters to simplified Chinese characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "for filename in positiveFiles:\n",
    "    text = \"\"\n",
    "    with codecs.open(filename, \"rb\") as doc_file:\n",
    "        for line in doc_file:\n",
    "            try:\n",
    "                line = line.decode(\"GB2312\")\n",
    "            except:\n",
    "                continue\n",
    "            text+=Converter('zh-hans').convert(line)# Convert from traditional to simplified Chinese\n",
    "\n",
    "            text = text.replace(\"\\n\", \"\")\n",
    "            text = text.replace(\"\\r\", \"\")\n",
    "    documents.append((text, \"pos\"))\n",
    "\n",
    "for filename in negativeFiles:\n",
    "    text = \"\"\n",
    "    with codecs.open(filename, \"rb\") as doc_file:\n",
    "        for line in doc_file:\n",
    "            try:\n",
    "                line = line.decode(\"GB2312\")\n",
    "            except:\n",
    "                continue\n",
    "            text+=Converter('zh-hans').convert(line)# Convert from traditional to simplified Chinese\n",
    "\n",
    "            text = text.replace(\"\\n\", \"\")\n",
    "            text = text.replace(\"\\r\", \"\")\n",
    "    documents.append((text, \"neg\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional step to save/load the documents as pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5916\n",
      "('优点;有电梯(4层开封宾馆连电梯都没有),晚上好像没有小姐电话骚扰(开封宾馆骚扰到最后干脆和按摩小姐电话聊天起来了)缺点;房间象招待所,设备老化,能算3星?价格偏高,性价比不好!门口乱哄哄的,感觉不好!', 'neg')\n"
     ]
    }
   ],
   "source": [
    "# Uncomment those two lines to save/load the documents for later use since the step above takes a while\n",
    "# __pickleStuff(\"./data/chinese_sentiment_corpus.p\", documents)\n",
    "# documents = __loadStuff(\"./data/chinese_sentiment_corpus.p\")\n",
    "print(len(documents))\n",
    "print(documents[4000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## shuffle the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the input and output for the model\n",
    "Each input (hotel review) will be a list of tokens, output will be one token(\"pos\" or \"neg\"). The stopwords are not removed here since the dataset is relative small and removing the stop words are not saving much traing time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize only\n",
    "totalX = []\n",
    "totalY = [str(doc[1]) for doc in documents]\n",
    "for doc in documents:\n",
    "    seg_list = jieba.cut(doc[0], cut_all=False)\n",
    "    seg_list = list(seg_list)\n",
    "    totalX.append(seg_list)\n",
    "\n",
    "\n",
    "#Switch to below code to experiment with removing stop words\n",
    "# Tokenize and remove stop words\n",
    "# totalX = []\n",
    "# totalY = [str(doc[1]) for doc in documents]\n",
    "# stopwords = [ line.rstrip() for line in codecs.open('./data/chinese_stop_words.txt',\"r\", encoding=\"utf-8\") ]\n",
    "# for doc in documents:\n",
    "#     seg_list = jieba.cut(doc[0], cut_all=False)\n",
    "#     seg_list = list(seg_list)\n",
    "#     Uncomment below code to experiment with removing stop words\n",
    "#     final =[]\n",
    "#     for seg in seg_list:\n",
    "#         if seg not in stopwords:\n",
    "#             final.append(seg)\n",
    "#     totalX.append(final)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize distribution of sentence length\n",
    "Decide the max input sequence, here we cover up to 60% sentences. The longer input sequence, the more training time will take, but could improve  prediction accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length is:  1804\n",
      "60% cover length up to:  68\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f602850ae10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import pylab as pl\n",
    "h = sorted([len(sentence) for sentence in totalX])\n",
    "maxLength = h[int(len(h) * 0.60)]\n",
    "print(\"Max length is: \",h[len(h)-1])\n",
    "print(\"60% cover length up to: \",maxLength)\n",
    "h = h[:5000]\n",
    "fit = stats.norm.pdf(h, np.mean(h), np.std(h))  #this is a fitting indeed\n",
    "\n",
    "pl.plot(h,fit,'-o')\n",
    "pl.hist(h,normed=True)      #use this to draw histogram of your data\n",
    "pl.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Words to number tokens, padding\n",
    "Pad input sequence to max input length if it is shorter\n",
    "\n",
    "\n",
    "Save the input tokenizer, since we need to use the same tokenizer for our new predition data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input vocab_size: 22123\n"
     ]
    }
   ],
   "source": [
    "totalX = [\" \".join(wordslist) for wordslist in totalX]  # Keras Tokenizer expect the words tokens to be seperated by space \n",
    "input_tokenizer = Tokenizer(30000) # Initial vocab size\n",
    "input_tokenizer.fit_on_texts(totalX)\n",
    "vocab_size = len(input_tokenizer.word_index) + 1\n",
    "print(\"input vocab_size:\",vocab_size)\n",
    "totalX = np.array(pad_sequences(input_tokenizer.texts_to_sequences(totalX), maxlen=maxLength))\n",
    "__pickleStuff(\"./data/input_tokenizer_chinese.p\", input_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output, array of 0s and 1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output vocab_size: 3\n"
     ]
    }
   ],
   "source": [
    "target_tokenizer = Tokenizer(3)\n",
    "target_tokenizer.fit_on_texts(totalY)\n",
    "print(\"output vocab_size:\",len(target_tokenizer.word_index) + 1)\n",
    "totalY = np.array(target_tokenizer.texts_to_sequences(totalY)) -1\n",
    "totalY = totalY.reshape(totalY.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 0, 0, 1, 1, 0, 0])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totalY[40:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turn output 0s and 1s to categories(one-hot vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalY = to_categorical(totalY, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totalY[40:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dimen = totalY.shape[1] # which is 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save meta data for later predition\n",
    "maxLength: the input sequence length\n",
    "\n",
    "vocab_size: Input vocab size\n",
    "\n",
    "output_dimen: which is 2 in this example (pos or neg)\n",
    "\n",
    "sentiment_tag: either [\"neg\",\"pos\"] or [\"pos\",\"neg\"] matching the target tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_reverse_word_index = {v: k for k, v in list(target_tokenizer.word_index.items())}\n",
    "sentiment_tag = [target_reverse_word_index[1],target_reverse_word_index[2]] \n",
    "metaData = {\"maxLength\":maxLength,\"vocab_size\":vocab_size,\"output_dimen\":output_dimen,\"sentiment_tag\":sentiment_tag}\n",
    "__pickleStuff(\"./data/meta_sentiment_chinese.p\", metaData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Model, train and save it\n",
    "The training data is logged to Tensorboard, we can look at it by cd into directory \n",
    "\n",
    "\"./Graph/sentiment_chinese\" and run\n",
    "\n",
    "\n",
    "\"python -m tensorflow.tensorboard --logdir=.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 68, 256)           5663488   \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 68, 256)           393984    \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, 256)               393984    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 6,451,970\n",
      "Trainable params: 6,451,970\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 5324 samples, validate on 592 samples\n",
      "Epoch 1/20\n",
      "5324/5324 [==============================] - 49s 9ms/step - loss: 0.6808 - acc: 0.5640 - val_loss: 0.5026 - val_acc: 0.7770\n",
      "Epoch 2/20\n",
      "5324/5324 [==============================] - 49s 9ms/step - loss: 0.5421 - acc: 0.7483 - val_loss: 0.4294 - val_acc: 0.8074\n",
      "Epoch 3/20\n",
      "5324/5324 [==============================] - 48s 9ms/step - loss: 0.4263 - acc: 0.8276 - val_loss: 0.4619 - val_acc: 0.8041\n",
      "Epoch 4/20\n",
      "5324/5324 [==============================] - 49s 9ms/step - loss: 0.3507 - acc: 0.8650 - val_loss: 0.3194 - val_acc: 0.8834\n",
      "Epoch 5/20\n",
      "5324/5324 [==============================] - 49s 9ms/step - loss: 0.2998 - acc: 0.8866 - val_loss: 0.2837 - val_acc: 0.8936\n",
      "Epoch 6/20\n",
      "5324/5324 [==============================] - 49s 9ms/step - loss: 0.2695 - acc: 0.9006 - val_loss: 0.2839 - val_acc: 0.8986\n",
      "Epoch 7/20\n",
      "5324/5324 [==============================] - 49s 9ms/step - loss: 0.2174 - acc: 0.9222 - val_loss: 0.2801 - val_acc: 0.8936\n",
      "Epoch 8/20\n",
      "5324/5324 [==============================] - 49s 9ms/step - loss: 0.1895 - acc: 0.9324 - val_loss: 0.2746 - val_acc: 0.8986\n",
      "Epoch 9/20\n",
      "5324/5324 [==============================] - 49s 9ms/step - loss: 0.1660 - acc: 0.9369 - val_loss: 0.2981 - val_acc: 0.8885\n",
      "Epoch 10/20\n",
      "5324/5324 [==============================] - 49s 9ms/step - loss: 0.1526 - acc: 0.9429 - val_loss: 0.3024 - val_acc: 0.9003\n",
      "Epoch 11/20\n",
      "5324/5324 [==============================] - 49s 9ms/step - loss: 0.1259 - acc: 0.9551 - val_loss: 0.3632 - val_acc: 0.9003\n",
      "Epoch 12/20\n",
      "5324/5324 [==============================] - 49s 9ms/step - loss: 0.1337 - acc: 0.9482 - val_loss: 0.3948 - val_acc: 0.8986\n",
      "Epoch 13/20\n",
      "5324/5324 [==============================] - 49s 9ms/step - loss: 0.1133 - acc: 0.9591 - val_loss: 0.3517 - val_acc: 0.8868\n",
      "Epoch 14/20\n",
      "5324/5324 [==============================] - 49s 9ms/step - loss: 0.0905 - acc: 0.9634 - val_loss: 0.3895 - val_acc: 0.8632\n",
      "Epoch 15/20\n",
      "5324/5324 [==============================] - 49s 9ms/step - loss: 0.0990 - acc: 0.9637 - val_loss: 0.3961 - val_acc: 0.8801\n",
      "Epoch 16/20\n",
      "5324/5324 [==============================] - 49s 9ms/step - loss: 0.0929 - acc: 0.9624 - val_loss: 0.3559 - val_acc: 0.8970\n",
      "Epoch 17/20\n",
      "5324/5324 [==============================] - 49s 9ms/step - loss: 0.0962 - acc: 0.9649 - val_loss: 0.4281 - val_acc: 0.8885\n",
      "Epoch 18/20\n",
      "5324/5324 [==============================] - 49s 9ms/step - loss: 0.0849 - acc: 0.9651 - val_loss: 0.4243 - val_acc: 0.8953\n",
      "Epoch 19/20\n",
      "5324/5324 [==============================] - 49s 9ms/step - loss: 0.0747 - acc: 0.9694 - val_loss: 0.4803 - val_acc: 0.8902\n",
      "Epoch 20/20\n",
      "5324/5324 [==============================] - 49s 9ms/step - loss: 0.0768 - acc: 0.9692 - val_loss: 0.4465 - val_acc: 0.9020\n",
      "Saved model!\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 256\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim,input_length = maxLength))\n",
    "# Each input would have a size of (maxLength x 256) and each of these 256 sized vectors are fed into the GRU layer one at a time.\n",
    "# All the intermediate outputs are collected and then passed on to the second GRU layer.\n",
    "model.add(GRU(256, dropout=0.9, return_sequences=True))\n",
    "# Using the intermediate outputs, we pass them to another GRU layer and collect the final output only this time\n",
    "model.add(GRU(256, dropout=0.9))\n",
    "# The output is then sent to a fully connected layer that would give us our final output_dim classes\n",
    "model.add(Dense(output_dimen, activation='softmax'))\n",
    "# We use the adam optimizer instead of standard SGD since it converges much faster\n",
    "tbCallBack = TensorBoard(log_dir='./Graph/sentiment_chinese', histogram_freq=0,\n",
    "                            write_graph=True, write_images=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "model.fit(totalX, totalY, validation_split=0.1, batch_size=32, epochs=20, verbose=1, callbacks=[tbCallBack])\n",
    "model.save('./data/sentiment_chinese_model.HDF5')\n",
    "\n",
    "print(\"Saved model!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below are prediction code\n",
    "Function to load the meta data and the model we just trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None\n",
    "sentiment_tag = None\n",
    "maxLength = None\n",
    "def loadModel():\n",
    "    global model, sentiment_tag, maxLength\n",
    "    metaData = __loadStuff(\"./data/meta_sentiment_chinese.p\")\n",
    "    maxLength = metaData.get(\"maxLength\")\n",
    "    vocab_size = metaData.get(\"vocab_size\")\n",
    "    output_dimen = metaData.get(\"output_dimen\")\n",
    "    sentiment_tag = metaData.get(\"sentiment_tag\")\n",
    "    embedding_dim = 256\n",
    "    if model is None:\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(vocab_size, embedding_dim, input_length=maxLength))\n",
    "        # Each input would have a size of (maxLength x 256) and each of these 256 sized vectors are fed into the GRU layer one at a time.\n",
    "        # All the intermediate outputs are collected and then passed on to the second GRU layer.\n",
    "        model.add(GRU(256, dropout=0.9, return_sequences=True))\n",
    "        # Using the intermediate outputs, we pass them to another GRU layer and collect the final output only this time\n",
    "        model.add(GRU(256, dropout=0.9))\n",
    "        # The output is then sent to a fully connected layer that would give us our final output_dim classes\n",
    "        model.add(Dense(output_dimen, activation='softmax'))\n",
    "        # We use the adam optimizer instead of standard SGD since it converges much faster\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        model.load_weights('./data/sentiment_chinese_model.HDF5')\n",
    "        model.summary()\n",
    "    print(\"Model weights loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to convert sentence to model input, and predict result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findFeatures(text):\n",
    "    text=Converter('zh-hans').convert(text)\n",
    "    text = text.replace(\"\\n\", \"\")\n",
    "    text = text.replace(\"\\r\", \"\") \n",
    "    seg_list = jieba.cut(text, cut_all=False)\n",
    "    seg_list = list(seg_list)\n",
    "    text = \" \".join(seg_list)\n",
    "    textArray = [text]\n",
    "    input_tokenizer_load = __loadStuff(\"./data/input_tokenizer_chinese.p\")\n",
    "    textArray = np.array(pad_sequences(input_tokenizer_load.texts_to_sequences(textArray), maxlen=maxLength))\n",
    "    return textArray\n",
    "def predictResult(text):\n",
    "    if model is None:\n",
    "        print(\"Please run \\\"loadModel\\\" first.\")\n",
    "        return None\n",
    "    features = findFeatures(text)\n",
    "    predicted = model.predict(features)[0] # we have only one sentence to predict, so take index 0\n",
    "    predicted = np.array(predicted)\n",
    "    probab = predicted.max()\n",
    "    predition = sentiment_tag[predicted.argmax()]\n",
    "    return predition, probab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calling the load model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 68, 256)           5663488   \n",
      "_________________________________________________________________\n",
      "gru_5 (GRU)                  (None, 68, 256)           393984    \n",
      "_________________________________________________________________\n",
      "gru_6 (GRU)                  (None, 256)               393984    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 6,451,970\n",
      "Trainable params: 6,451,970\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model weights loaded!\n"
     ]
    }
   ],
   "source": [
    "loadModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try some new comments, feel free to try your own\n",
    "The result tuple consists the predicted result and likehood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('neg', 0.6844894)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictResult(\"还好，床很大而且很干净，前台很友好，很满意，下次还来。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('neg', 0.92276686)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictResult(\"床上有污渍，房间太挤不透气，空调不怎么好用。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('pos', 0.9897489)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictResult(\"房间有点小但是设备还齐全，没有异味。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('pos', 0.93582195)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictResult(\"房间还算干净，一般般吧，短住还凑合。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('pos', 0.5725031)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictResult(\"开始不太满意，前台好说话换了一间，房间很干净没有异味。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('neg', 0.67216)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictResult(\"你是个SB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
